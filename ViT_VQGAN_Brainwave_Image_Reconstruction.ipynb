{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ejn8iRwLjzY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/out_files.zip\n",
        "#!unzip /content/drive/MyDrive/MindBigData-Imagenet-IN.zip"
      ],
      "metadata": {
        "id": "NcfWETGSj2E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "!pip install omegaconf einops\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "!mkdir vit_vqgan\n",
        "%cd vit_vqgan\n",
        "!gdown 1DbHEBNzjefNfwG0AKvYKB64if5Usua2n\n",
        "!gdown 1-9INRFzvxDlQxyLX3fGA9ZnL3HXYyeTf\n",
        "!gdown 1HzNvpeqvUTHz9tQOiV6G2r5sHKKhKQUZ\n",
        "!gdown 1syv0t3nAJ-bETFgFpztw9cPXghanUaM6\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "WTyGL_b_fcUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1DbHEBNzjefNfwG0AKvYKB64if5Usua2n\n",
        "!gdown 1-9INRFzvxDlQxyLX3fGA9ZnL3HXYyeTf\n",
        "!gdown 1HzNvpeqvUTHz9tQOiV6G2r5sHKKhKQUZ\n",
        "!gdown 1syv0t3nAJ-bETFgFpztw9cPXghanUaM6"
      ],
      "metadata": {
        "id": "R4ZJg3xPlA73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import sys,os\n",
        "os.chdir(\"/content/vit_vqgan\")\n",
        "sys.path.append(\"/content/vit_vqgan\")\n",
        "\n",
        "import io\n",
        "import re\n",
        "import PIL\n",
        "import fnmatch\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.utils import save_image\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import save_image\n",
        "import gc\n",
        "import cv2\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from vitvqgan import ViTVQ\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics, model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "Oz9rJmOMfUq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions\n",
        "def show_ims(recon,original):\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "    Image1 = np.array(original)\n",
        "    Image2 = np.array(recon)\n",
        "    fig.add_subplot(1, 2, 1)\n",
        "      \n",
        "    plt.imshow(Image1)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Original\")\n",
        "    fig.add_subplot(1,2,2)\n",
        "      \n",
        "    plt.imshow(Image2)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Reconstructed\")\n",
        "\n",
        "def download_image(url):\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    return PIL.Image.open(io.BytesIO(resp.content))\n",
        "\n",
        "def preprocess(img):        \n",
        "    #r = 256 / s\n",
        "    #s = (round(r * img.size[1]), round(r * img.size[0]))\n",
        "    #img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
        "    #img = TF.center_crop(img, output_size=2 * [256])\n",
        "    img = img.resize((256,256))\n",
        "    img_t = T.ToTensor()(img)\n",
        "    return img, img_t\n",
        "\n",
        "to_Pil=T.ToPILImage()"
      ],
      "metadata": {
        "id": "U9vbcHUWgAMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TSParti(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Simplest classifier/regressor. Can be either regressor or classifier because the output does not include\n",
        "    softmax. Concatenates final layer embeddings and uses 0s to ignore padding embeddings in final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ts_dim=5, ts_seq_len=360, nhead=4, num_encoder_layers=3, vae_path='./imagenet_vitvq_base.ckpt'):\n",
        "        super(TSParti, self).__init__()\n",
        "\n",
        "        self.d_model = 32\n",
        "        im_seq_len = 1024\n",
        "        n_embed = 8192\n",
        "        \n",
        "        self.nhead = nhead\n",
        "\n",
        "        self.ts_projection = torch.nn.Linear(ts_dim, self.d_model)\n",
        "        self.transformer_model = torch.nn.Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, batch_first=True, d_model=self.d_model)\n",
        "        self.start_token = torch.nn.Parameter(torch.randn(1,1,self.d_model))\n",
        "        #self.end_token = torch.nn.Parameter(torch.randn(1,1,self.d_model))\n",
        "        self.image_token_embed = torch.nn.Embedding(n_embed, self.d_model)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(self.d_model, 0.1, max_len=360)\n",
        "        self.axial_height_pos = torch.nn.Parameter(torch.randn(im_seq_len, self.d_model))\n",
        "        self.axial_width_pos = torch.nn.Parameter(torch.randn(im_seq_len, self.d_model))\n",
        "\n",
        "        self.to_logits = torch.nn.Linear(self.d_model, n_embed, bias = False)\n",
        "        self.to_logits.weight = self.image_token_embed.weight\n",
        "\n",
        "    def forward(self, src, tgt_codes, tgt_mask):\n",
        "        #training only!\n",
        "        #src: bs, ts_seq_len, ts_dim\n",
        "        #tgt: bs, im_seq_len, im_dim\n",
        "        bs = src.shape[0]\n",
        "        \n",
        "        st = self.start_token.repeat(bs, 1, 1)\n",
        "\n",
        "        tgt_codes, labels = tgt_codes[:, :-1], tgt_codes\n",
        "        tgt = self.image_token_embed(tgt_codes)\n",
        "        tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "        axial_pos_emb = self.axial_width_pos.unsqueeze(0) + self.axial_height_pos.unsqueeze(1)\n",
        "        axial_pos_emb = axial_pos_emb.reshape(axial_pos_emb.shape[0]*axial_pos_emb.shape[1], axial_pos_emb.shape[2])\n",
        "        tgt = tgt + axial_pos_emb[:tgt_seq_len]\n",
        "        tgt = torch.cat([st, tgt], 1)\n",
        "        src = self.ts_projection(src) \n",
        "        enc_out = self.transformer_model.encoder(src)# * math.sqrt(self.d_model)\n",
        "        enc_out = self.pos_encoder(enc_out)\n",
        "        #enc_out = torch.cat([st, enc_out], 1)\n",
        "        dec_out = self.transformer_model.decoder(tgt, enc_out)#, tgt_mask)\n",
        "        logits = self.to_logits(dec_out)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "3ood63UY2fB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data\n",
        "def trim_brain_ts(brain_ts, cutoff):\n",
        "    if len(brain_ts) > cutoff:\n",
        "        cutoff_row_n = (len(brain_ts) - cutoff) // 2\n",
        "        brain_ts = brain_ts[cutoff_row_n:len(brain_ts)-cutoff_row_n]\n",
        "    return brain_ts\n",
        "\n",
        "class BrainwaveDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, new_csvs, new_imgs, tensor_path):\n",
        "        self.new_csvs = new_csvs\n",
        "        self.new_imgs = new_imgs\n",
        "        self.tss = torch.load(tensor_path)\n",
        "        self.mean = self.tss.mean(0).mean(0)\n",
        "        self.std = self.tss.std(0).std(0)\n",
        "        self.tss = (self.tss - self.mean) / self.std\n",
        "\n",
        "        self.tss = self.tss[:len(new_imgs)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.new_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      _, img = preprocess(Image.open(self.new_imgs[idx]).convert('RGB'))\n",
        "      #brain_ts = pd.read_csv(self.new_csvs[idx], index_col=0, header=None).T\n",
        "      #brain_ts = (brain_ts - np.min(brain_ts, 0)) / (np.max(brain_ts, 0) - np.min(brain_ts, 0))\n",
        "      src = self.tss[idx]\n",
        "      #src = torch.Tensor(trim_brain_ts(brain_ts, CUTOFF).to_numpy())\n",
        "      return {'src':src, 'img':img}"
      ],
      "metadata": {
        "id": "DsZModdogl-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_im(im):\n",
        "  save_image(im, 'im.png')\n",
        "  return Image.open(\"im.png\")"
      ],
      "metadata": {
        "id": "NwEyvF55jy5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64\n",
        "ts_seq_len = 360\n",
        "ts_dim = 5\n",
        "im_seq_len = 1024\n",
        "im_dim = 32\n",
        "\n",
        "CUTOFF = ts_seq_len\n",
        "\n",
        "nhead = 4\n",
        "num_encoder_layers = 3\n",
        "epochs = 1000\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tss_train_tensor_path = '/content/drive/MyDrive/TSS Tensors/train_tss.pt'\n",
        "tss_test_tensor_path = '/content/drive/MyDrive/TSS Tensors/test_tss.pt'\n",
        "codes_train_tensor_path = '/content/drive/MyDrive/TSS Tensors/train_codes.pt'\n",
        "codes_test_tensor_path = '/content/drive/MyDrive/TSS Tensors/test_codes.pt'\n",
        "new_imgs_csvs_labels = '/content/drive/MyDrive/TSS Tensors/new_imgs_csvs_labels.pt'\n",
        "\n",
        "new_imgs, new_csvs, labels = torch.load(new_imgs_csvs_labels)\n",
        "\n",
        "#new_imgs = [new_imgs[0]]"
      ],
      "metadata": {
        "id": "MwG-_BLAfWyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = {'dim': 768, 'depth': 12, 'heads': 12, 'mlp_dim': 3072}\n",
        "decoder = {'dim': 768, 'depth': 12, 'heads': 12, 'mlp_dim': 3072}\n",
        "quantizer = {'embed_dim': 32, 'n_embed': 8192}\n",
        "vae_path = './imagenet_vitvq_base.ckpt'\n",
        "tsparti = TSParti(ts_dim=ts_dim, ts_seq_len=ts_seq_len, nhead=nhead, num_encoder_layers=num_encoder_layers).to(device)\n",
        "vae = ViTVQ(256, 8, encoder, decoder, quantizer, path=vae_path).to(device)#.cuda()\n",
        "for param in vae.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "MLAXnxMTZvQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_imgs_train, new_imgs_test, new_csvs_train, new_csvs_test = train_test_split(new_imgs, new_csvs, test_size=0.1, random_state=42)\n",
        "new_imgs_train, new_imgs_test, new_csvs_train, new_csvs_test = new_imgs, new_imgs, new_csvs, new_csvs\n",
        "ds_train = BrainwaveDataset(new_csvs_train, new_imgs_train, tss_train_tensor_path)\n",
        "ds_test = BrainwaveDataset(new_csvs_test, new_imgs_test, tss_test_tensor_path)\n",
        "\n",
        "train_dataloader = DataLoader(ds_train, batch_size=bs, shuffle=False)\n",
        "test_dataloader = DataLoader(ds_test, batch_size=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "8oOmAkOduH56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_codes = []\n",
        "for i, samp in enumerate(train_dataloader):\n",
        "    #src = samp['src'].to(device)\n",
        "    img = samp['img'].to(device)\n",
        "    #src = torch.ones_like(src).to(device)\n",
        "    with torch.no_grad():\n",
        "      tgt_codes = vae.encode_codes(img)\n",
        "      all_codes.append(tgt_codes)\n",
        "\n",
        "all_codes = torch.cat(all_codes, 0)\n",
        "torch.save(all_codes, '/content/drive/MyDrive/TSS Tensors/train_codes.pt')"
      ],
      "metadata": {
        "id": "bZXeJnHXehmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_codes = []\n",
        "for i, samp in enumerate(test_dataloader):\n",
        "    #src = samp['src'].to(device)\n",
        "    img = samp['img'].to(device)\n",
        "    #src = torch.ones_like(src).to(device)\n",
        "    with torch.no_grad():\n",
        "      tgt_codes = vae.encode_codes(img)\n",
        "      all_codes.append(tgt_codes)\n",
        "\n",
        "all_codes = torch.cat(all_codes, 0)\n",
        "torch.save(all_codes, '/content/drive/MyDrive/TSS Tensors/test_codes.pt')"
      ],
      "metadata": {
        "id": "AnVpJzHHkpOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterator = iter(train_dataloader)"
      ],
      "metadata": {
        "id": "spTkhV3P6wn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(tsparti.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', min_lr=1e-7, patience=100, threshold=0.001)"
      ],
      "metadata": {
        "id": "JTvXsy1qFN7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tsparti = tsparti.to(device)\n",
        "vae = vae.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i, samp in enumerate(train_dataloader):\n",
        "    src = samp['src'].to(device)\n",
        "    img = samp['img'].to(device)\n",
        "    #src = torch.ones_like(src).to(device)\n",
        "    with torch.no_grad():\n",
        "      tgt_codes = vae.encode_codes(img)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    tgt_mask = torch.triu(torch.ones(im_seq_len, im_seq_len) * float('-inf'), diagonal=1).repeat(bs*nhead, 1, 1).to(device)\n",
        "    #tgt_mask = torch.ones(im_seq_len, im_seq_len).repeat(src.shape[0]*nhead, 1, 1).to(device)\n",
        "    logits = tsparti(src, tgt_codes, tgt_mask).to(device)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.permute(0,2,1), tgt_codes)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step(loss)\n",
        "    lr = [el['lr'] for el in optimizer.param_groups][0]\n",
        "    print(f'Epoch: {epoch}; Step: {i}; loss: {loss}; lr: {lr}')\n",
        "# tgt - label; dec_out - preds\n",
        "#with torch.no_grad():\n",
        "#  vae_decoded = vae.decoder(dec_out[:, :1024, :])\n",
        "#Image.fromarray(((vae_decoded[0].permute(1,2,0) + 0.5) * 255).to(torch.uint8).detach().numpy())"
      ],
      "metadata": {
        "id": "i7ohN8-_6vrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log(t, eps = 1e-20):\n",
        "    return torch.log(t + eps)\n",
        "  \n",
        "def gumbel_noise(t):\n",
        "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
        "    return -log(-log(noise))\n",
        "\n",
        "def gumbel_sample(t, temperature = 1., dim = -1):\n",
        "    return ((t / temperature) + gumbel_noise(t)).argmax(dim = dim)\n",
        "\n",
        "def top_k(logits, thres = 0.5):\n",
        "    lg = logits[0][0]\n",
        "    num_logits = lg.shape[-1]\n",
        "    k = max(int((1 - thres) * num_logits), 1)\n",
        "    val, ind = torch.topk(lg, k)\n",
        "    probs = torch.full_like(lg, float('-inf'))\n",
        "    probs[ind] = val\n",
        "    probs = probs.unsqueeze(0).unsqueeze(0)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "18THlKG7h3jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "def generate(tsparti, src, tgt):\n",
        "  #bs = src.shape[0]\n",
        "  #seq_len = tgt.shape[1]\n",
        "  #tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).repeat(1*nhead, 1, 1).to(device)\n",
        "  #tgt_mask = torch.ones(seq_len, seq_len).repeat(bs*nhead, 1, 1).to(device)\n",
        "  src = tsparti.ts_projection(src)\n",
        "  enc_out = tsparti.transformer_model.encoder(src)# * math.sqrt(tsparti.d_model)\n",
        "  enc_out = tsparti.pos_encoder(enc_out)\n",
        "  dec_out = tsparti.transformer_model.decoder(tgt, enc_out)\n",
        "  logits = tsparti.to_logits(dec_out)\n",
        "  return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #'cpu'\n",
        "tsparti = tsparti.to(device)\n",
        "vae = vae.to(device)\n",
        "\n",
        "tgt = copy.deepcopy(tsparti.start_token.to(device))#torch.empty((1, 0), device = device, dtype = torch.long)#copy.deepcopy(tsparti.start_token.to(device))\n",
        "with torch.no_grad():\n",
        "  codes = tsparti.to_logits(tsparti.start_token).argmax(2)\n",
        "print(codes)\n",
        "for i, samp in enumerate(train_dataloader):\n",
        "  src = samp['src'][0].unsqueeze(0).to(device)\n",
        "  img = samp['img'][0].to(device)\n",
        "  #src = torch.ones_like(src).to(device)\n",
        "  for j in tqdm(range(0, im_seq_len)):\n",
        "    with torch.no_grad():\n",
        "      tgt = generate(tsparti, src, tgt)\n",
        "      tgt = tgt[:, -1, :].unsqueeze(1)\n",
        "      #token_code = tgt.argmax(2)\n",
        "      filtered_logits = top_k(tgt, thres = 0.9)\n",
        "      token_code = gumbel_sample(filtered_logits, temperature = 1, dim = -1)\n",
        "      codes = torch.cat([codes, token_code], 1)\n",
        "      tgt = vae.quantizer.embedding(codes)\n",
        "      tgt = vae.quantizer.norm(tgt)\n",
        "      \n",
        "      #tgt = torch.cat([tgt, token], 1).to(device)\n",
        "  break\n",
        "\n",
        "codes = codes[:, 1:]\n",
        "#dec_img = vae.decode(tgt[:, 1:, :])*255\n",
        "#img = (dec_img[0].detach().cpu().permute(1,2,0).to(torch.uint8).numpy())\n",
        "#img = Image.fromarray(img)"
      ],
      "metadata": {
        "id": "3T-umjsz_gO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "with torch.no_grad():\n",
        "  emb = vae.decode_codes(codes)\n",
        "  #dec = vae.decode(emb)\n",
        "\n",
        "ipd.display(show_im(emb[0]))\n"
      ],
      "metadata": {
        "id": "FmljiyHqyqHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  emb = vae.decode_codes(tgt_codes)\n",
        "  #dec = vae.decode(emb)\n",
        "\n",
        "show_im(emb[0])"
      ],
      "metadata": {
        "id": "SMaBoVTdaBK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''imgs = glob.glob('/content/out_files/ILSVRC/Data/CLS-LOC/train/*/*.JPEG')\n",
        "csvs = glob.glob('/content/MindBigData-Imagenet/*.csv')\n",
        "\n",
        "with open('/content/WordReport-v1.04.txt') as f:\n",
        "  class_data = f.read()\n",
        "class_data = list(map(lambda x: x.split('\\t'), class_data.split('\\n')))[:-1]\n",
        "class_dict = {el[2]:el[0] for el in class_data}\n",
        "\n",
        "csv_ids = list(map(lambda x: x[59:].rsplit('_', 2)[0], csvs))\n",
        "img_ids = list(map(lambda x: x[55:-5], imgs))\n",
        "ids = np.unique([el_mp for el_mp in csv_ids if el_mp in img_ids])\n",
        "\n",
        "new_imgs, new_csvs = [], []\n",
        "for id in ids:\n",
        "  csv_matched = fnmatch.filter(csvs, '*' + id + '*.csv')\n",
        "  img_matched = fnmatch.filter(imgs, '*' + id + '.JPEG')\n",
        "  new_csvs += csv_matched\n",
        "  new_imgs += img_matched * len(csv_matched)\n",
        "  #break\n",
        "\n",
        "labels = [class_dict[re.findall('n[0-9]+', el)[0]] for el in new_csvs]\n",
        "new_csvs, new_imgs, labels = shuffle(new_csvs, new_imgs, labels, random_state=0)'''"
      ],
      "metadata": {
        "id": "FmVfAYgHhFmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''tss_train = torch.cat([ds_train[i]['src'].unsqueeze(0) for i in range(len(ds_train))], 0)\n",
        "torch.save(tss_train, tss_train_tensor_path)\n",
        "\n",
        "tss_test = torch.cat([ds_test[i]['src'].unsqueeze(0) for i in range(len(ds_test))], 0)\n",
        "torch.save(tss_test, tss_test_tensor_path)'''"
      ],
      "metadata": {
        "id": "JWo_6QPwSiCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''text_token_embeds, text_mask = self.encode_texts(texts, output_device = device)\n",
        "\n",
        "batch = text_token_embeds.shape[0]\n",
        "\n",
        "image_seq_len = self.image_encoded_dim ** 2\n",
        "\n",
        "image_tokens = torch.empty((batch, 0), device = device, dtype = torch.long)\n",
        "\n",
        "for _ in range(image_seq_len):\n",
        "    logits = self.forward_with_cond_scale(\n",
        "        text_token_embeds = text_token_embeds,\n",
        "        text_mask = text_mask,\n",
        "        image_token_ids = image_tokens\n",
        "    )[:, -1]\n",
        "\n",
        "    filtered_logits = top_k(logits, thres = filter_thres)\n",
        "    sampled = gumbel_sample(filtered_logits, temperature = temperature, dim = -1)\n",
        "\n",
        "    sampled = rearrange(sampled, 'b -> b 1')\n",
        "    image_tokens = torch.cat((image_tokens, sampled), dim = -1)\n",
        "\n",
        "image_tokens = rearrange(image_tokens, 'b (h w) -> b h w', h = self.image_encoded_dim)'''"
      ],
      "metadata": {
        "id": "r_9EspLL8Yla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install transformers==4.11.2\n",
        "!pip install --upgrade sentencepiece\n",
        "from transformers import T5Tokenizer, T5EncoderModel, T5Config\n",
        "name = 'google/t5-v1_1-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(name)\n",
        "encoded = tokenizer.batch_encode_plus(\n",
        "        ['Customize your Wells Fargo Debit Card. Choose from over 30 collegiate designs'],\n",
        "        return_tensors = \"pt\",\n",
        "        padding = 'longest',\n",
        "        max_length = 256,\n",
        "        truncation = True\n",
        "    )'''"
      ],
      "metadata": {
        "id": "Jf322sA9xjUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''gc.collect()\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "image_path=\"https://jw-webmagazine.com/wp-content/uploads/2020/03/Kimetsu-no-YaibaDemon-Slayer.jpg\"\n",
        "if \"https\" in image_path:\n",
        "  original=download_image(image_path)\n",
        "else:\n",
        "  if os.path.exists(image_path):\n",
        "    original=Image.open(image_path)\n",
        "  else:\n",
        "    print(\"Please check the image path\")\n",
        "\n",
        "encoder = {'dim': 768, 'depth': 12,\n",
        "           'heads': 12, 'mlp_dim': 3072}\n",
        "decoder = {'dim': 768, 'depth': 12,\n",
        "           'heads': 12, 'mlp_dim': 3072}\n",
        "quantizer = {'embed_dim': 32, 'n_embed': 8192}\n",
        "\n",
        "image, image_t = preprocess(original)\n",
        "image_t = image_t#.cuda()\n",
        "clear_output()\n",
        "model = ViTVQ(256, 8, encoder, decoder, quantizer, path='./imagenet_vitvq_base.ckpt')#.cuda()\n",
        "recon, _ = model(image_t)\n",
        "recon = to_Pil(recon.squeeze(0))#.squeeze(0).permute(1,2,0).detach().cpu() * 255\n",
        "#recon = recon.to(torch.uint8)\n",
        "\n",
        "show_ims(recon,image)\n",
        "print(\"original saved at vit_vqgan/original.png \")\n",
        "print(\"reconstructed saved at vit_vqgan/reconstructed.png \")'''"
      ],
      "metadata": {
        "id": "XNEom9peX_t1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}