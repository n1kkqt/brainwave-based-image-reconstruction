{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evtPfbaSJwh3",
        "outputId": "b69af7cd-d88c-4144-bde9-f5348dcfe287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w40yexyC0OKu"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrjjUIpfBi_e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import math\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7FKKikF0OLF"
      },
      "source": [
        "## Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-3pqkjJHjiIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE8O8EKjV7Iw"
      },
      "source": [
        "def gen_nopeek_mask(length):\n",
        "    mask = torch.triu(torch.ones(length, length) * float('-inf'), diagonal=1)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainwaveDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, codes_tensor_path, tss_tensor_path):\n",
        "        self.tss = torch.load(tss_tensor_path, map_location=torch.device('cpu'))[0:1000]\n",
        "        self.codes = torch.load(codes_tensor_path, map_location=torch.device('cpu'))[0:1000]\n",
        "        self.min = self.tss.min(0).values.min(0).values\n",
        "        self.max = self.tss.max(0).values.max(0).values\n",
        "        #self.tss = (self.tss - self.min) / (self.max - self.min)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tss.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      src = self.tss[idx]\n",
        "      img = self.codes[idx]\n",
        "      return {'src':src, 'tgt':img}\n",
        "\n",
        "# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.src_embedding = nn.Linear(SRC_INPUT_DIM, ENC_EMB_DIM)\n",
        "        self.tgt_embedding = nn.Embedding(TGT_INPUT_DIM, ENC_EMB_DIM)\n",
        "        self.transformer = nn.Transformer(nhead=NHEAD, num_encoder_layers=N_ENC_LAYERS, d_model=ENC_EMB_DIM)\n",
        "        self.linear = nn.Linear(ENC_EMB_DIM, TGT_INPUT_DIM)\n",
        "        pos_dropout = 0.1\n",
        "        self.max_seq_length = 1024\n",
        "        self.pos_enc = PositionalEncoding(ENC_EMB_DIM, pos_dropout, self.max_seq_length)\n",
        "    \n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5, tgt_mask=None, train=True):\n",
        "        src_emb = self.pos_enc(self.src_embedding(src).permute(1,0,2) * math.sqrt(ENC_EMB_DIM))\n",
        "        tgt_emb = self.pos_enc(self.tgt_embedding(tgt).permute(1,0,2) * math.sqrt(ENC_EMB_DIM))\n",
        "\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "        if train:\n",
        "          iterations = self.max_seq_length-1\n",
        "        else:\n",
        "          iterations = self.max_seq_length\n",
        "        \n",
        "        if use_teacher_forcing:\n",
        "\n",
        "          outs = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
        "          outs = self.linear(outs)\n",
        "\n",
        "        else:\n",
        "\n",
        "          outs = torch.empty(0, bs, ENC_EMB_DIM).to(device)\n",
        "\n",
        "          src_emb = self.pos_enc(self.src_embedding(src).permute(1,0,2) * math.sqrt(ENC_EMB_DIM))\n",
        "          tgt_emb = self.pos_enc(self.tgt_embedding(tgt).permute(1,0,2) * math.sqrt(ENC_EMB_DIM))\n",
        "          memory = self.transformer.encoder(src_emb)\n",
        "\n",
        "          for _ in tqdm(range(iterations)):\n",
        "            out = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)[-1:]\n",
        "            \n",
        "            with torch.no_grad():\n",
        "              outs = torch.cat([outs, out], 0)\n",
        "\n",
        "          outs = self.linear(outs)\n",
        "          \n",
        "        return outs"
      ],
      "metadata": {
        "id": "dQdWmYN4PaSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, scheduler, criterion, teacher_forcing_ratio=0.5, clip=0.5):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch['src'].to(device)\n",
        "        tgt = batch['tgt'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device)\n",
        "        output = model(src, tgt_inp, teacher_forcing_ratio=teacher_forcing_ratio, tgt_mask=tgt_mask)\n",
        "        #from_one_hot = torch.argmax(output, dim=2)\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        tgt_out = tgt_out.flatten()\n",
        "        loss = criterion(output, tgt_out)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "        epoch_loss += loss.item()\n",
        "        lr = [el['lr'] for el in optimizer.param_groups][0]\n",
        "        print(loss, lr)\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            tgt_mask = gen_nopeek_mask(tgt.shape[1]).to('cuda')\n",
        "            with torch.no_grad():\n",
        "              output = model(src, tgt, 0, tgt_mask=tgt_mask) #turn off teacher forcing\n",
        "            #from_one_hot = torch.argmax(output, dim=2)\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            \n",
        "            tgt = tgt[:, 1:].flatten()\n",
        "            if tgt.shape[0] == output.shape[0]:\n",
        "              loss = criterion(output, tgt)\n",
        "\n",
        "              epoch_loss += loss.item()\n",
        "              print(loss)\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "KEfo8m4ttWeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_INPUT_DIM = 5\n",
        "TGT_INPUT_DIM = 8192\n",
        "\n",
        "ENC_EMB_DIM = 32\n",
        "NHEAD = 4\n",
        "N_ENC_LAYERS = 2\n",
        "\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "N_EPOCHS = 3\n",
        "CLIP = 1"
      ],
      "metadata": {
        "id": "RSXYAoRnAG1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "tss_train_tensor_path = '/content/drive/MyDrive/TSS Tensors/train_tss.pt'\n",
        "tss_test_tensor_path = '/content/drive/MyDrive/TSS Tensors/test_tss.pt'\n",
        "codes_train_tensor_path = '/content/drive/MyDrive/TSS Tensors/train_codes.pt'\n",
        "codes_test_tensor_path = '/content/drive/MyDrive/TSS Tensors/test_codes.pt'\n",
        "new_imgs_csvs_labels = '/content/drive/MyDrive/TSS Tensors/new_imgs_csvs_labels.pt'\n",
        "\n",
        "ds_train = BrainwaveDataset(codes_train_tensor_path, tss_train_tensor_path)\n",
        "ds_test = BrainwaveDataset(codes_test_tensor_path, tss_test_tensor_path)\n",
        "\n",
        "train_dataloader = DataLoader(ds_train, batch_size=bs, shuffle=True)\n",
        "test_dataloader = DataLoader(ds_test, batch_size=bs, shuffle=False)\n",
        "\n",
        "model = TransformerModel().to(device)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_normal_(p)"
      ],
      "metadata": {
        "id": "1oVYuqFYKHEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), betas=(0.9, 0.98), lr=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', min_lr=1e-7, patience=200, threshold=0.001)\n",
        "\n",
        "for epoch in range(0, N_EPOCHS):\n",
        "  train(model, train_dataloader, optimizer, scheduler, criterion, teacher_forcing_ratio=1-epoch/(N_EPOCHS-1), clip=CLIP)"
      ],
      "metadata": {
        "id": "Q-IbW2W-SWv2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78960938-498d-4b0d-e044-ff056c3cdcf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9.0213, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(8.9496, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(8.2296, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(7.7305, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(7.3608, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(7.0021, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(6.6864, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(6.4366, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(6.2574, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(6.1425, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(6.0624, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(6.0195, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9817, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9720, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9683, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9513, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9489, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9466, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9557, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9287, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9590, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9551, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9327, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9618, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9587, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9666, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9506, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9559, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9710, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9684, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9508, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9490, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9552, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9428, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9432, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9453, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9436, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9554, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9253, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9668, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9499, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9252, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9388, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9376, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9967, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9672, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9520, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9644, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9401, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9306, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9394, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9316, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9420, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9244, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9569, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9542, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9775, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9542, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9471, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9423, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9145, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9504, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9285, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n",
            "tensor(5.9510, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1023/1023 [02:17<00:00,  7.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.9413, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 19/1023 [00:02<01:58,  8.49it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4bb161d3b846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-24b2cc87f614>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, scheduler, criterion, teacher_forcing_ratio, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtgt_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_nopeek_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#from_one_hot = torch.argmax(output, dim=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-23dc9e5adc95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, teacher_forcing_ratio, tgt_mask, train)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    292\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                          memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.71 GiB already allocated; 3.75 MiB free; 13.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _, batch in enumerate(train_dataloader):\n",
        "\n",
        "  src = batch['src'].to(device)\n",
        "  tgt = batch['tgt'].to(device)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "  tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device)\n",
        "  output = model(src, tgt_inp, teacher_forcing_ratio=0, tgt_mask=tgt_mask)\n",
        "  break"
      ],
      "metadata": {
        "id": "tcYJQh6QxYKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, test_dataloader, criterion)"
      ],
      "metadata": {
        "id": "MlhnWmIb6aHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src: seq length x bs (indices) [<sos> word word word <eos> <pad> <pad> <pad>]\n",
        "# tgt: seq length x bs (indices) [<sos> word word word word <eos> <pad> <pad>]\n",
        "# src_key_padding_mask: bs x tgt seq length [0 0 0 0 0 1 1 1]\n",
        "# tgt_key_padding_mask: bs x tgt seq length [0 0 0 0 0 0 1 1]\n",
        "# memory_key_padding_mask = src_key_padding_mask\n",
        "# tgt_mask: tgt seq length x tgt seq length\n",
        "\n",
        "# src embedding: seq length x batch size x embedding dim\n",
        "#tgt_key_padding_mask.shape, tgt_inp.shape"
      ],
      "metadata": {
        "id": "SmY2cep--Cnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BL7yqmWYyMs"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}